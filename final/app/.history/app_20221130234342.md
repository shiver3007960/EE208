# <center>小型搜索引擎</center> #

<center>姓名：刘洺皓 学号：521030910014 班级：F2103001</center>

## 1.项目概述 ## 
&emsp;本项目旨在实现一个小型搜索引擎，其实现的基本原理如下：
* 利用python的urllib库和HTTP Base Handler库来实现网页爬取
* 利用lxml库构建网页树，解析网页内容，提取相关有用信息，使用jieba分词，亮点在于构建了一个具有打分机制的，兼容中英文，基本能够适配大量网站的对于图片的描述信息的提取的函数。
* 利用开源的Lucene引擎进行利用WhitespaceAnanlyzer实现中文分词建立索引和读取索引，以及一些增删改查和不同字符串搜索方式的实现。
*  构建了一个*Web*前端，实现了简单的主页，搜索结果呈现页和图片排版页，并拓展了一些如搜索历史查看，快捷方式增删，日期范围划定，加载页面呈现等功能。
* 接触并自己简略实现了*Simhash*文本查重,*Pagerank*,*Textrank*等算法。
*  再尝试寻找*Pagerank*的优化时，在*Google Scholar*上找到了一篇 *Adaptive methods for the computation of PageRank* 并咀嚼消化，最后也自己进行了实现。


## 2.项目环境 ##
本项目具体使用的工具如下:
* Docker中的ee208镜像(含Pylucene库)
* docker初始环境未安装的库:aiohttp(与asyncio实现异步协程爬虫),lxml,bitarray, func_set_timeout, jieba
* Python(Vscode)

## 3.项目片段 ##

### 3.1.1 中文的分词  ###
&emsp;利用jieba分词，可以利用 *cut_for_serach* 或者 *lcut_for_serach* 方法尽可能地切词，以提高召回率，同时在分析网页时也需要将原来的中文内容按照分词结果切开，中间留有空格，以期 _StandardAnalyzer_ 能够像英文切词一样将中文以空格切开，但是实际证明 _StandardAnalyzer_ 对待中文还是会一个一个字切开，所以这里改用 _WhitespaceAnalyzer_ 进行切词，提高对短语的完整性保留。

### 3.1.2 问题分析 ###

&emsp; 


### 3.1.2 代码与运行结果 ###
代码展示如下: 

```

```

运行结果如图:



#### 分析与反思： #### 


***


### 3.2.1 索引的建立 ### 
> &emsp; 

### 3.2.1 问题分析 ### 


&emsp; 
 
### 3.2.3 代码与运行结果 ###  
代码展示如下: 


```
```

运行结果如下：



#### 分析与思考: ####


 ***

### 3.3.1 网页解析 ### 
1. 清除 HTML 标签
   &emsp;显示 _nlkt_ 库已经 deprecate *clean_html* 方法，这里从 nlkt package 中沿用了原来的方法、
2. 为了提高匹配精确度，这里对 *jieba* 使用了停用词，以过滤常见词汇，停用词文件名称为 cnstopwords.txt, 在 instantiating *Textprocessing* 类时将会自动读取停用词并且将停用词作为类的静态变量给不同的实例化对象共享以节省空间。
3. 网页分析中考虑语言问题，这里使用 lxml 库来解析， 并运用 xpath 或 cssselect(需要 pip另外安装) 选择 <meta> 标签中的 lang属性， 若lang为en-US,则在分词时会不使用jieba而是将英文单词作为 raw material 传入 _WhitespaceAnalyzer_ , 若lang为 zh-CN ,则在分词时会使用jieba预留空格， 调用 *TextProcessor* 中的 `TokenizeChinese` 方法， 若lang中没有表明文件语言类型，那么将调用 *TextProcessor* 中的 `TokenizeMixedLan` 方法。
   * 查询 <meta> 中 lang，可用`"//head/meta[contains(@content,'charset=')]"`作为xpath.
   * *TextProcessor* 中的 `TokenizeChinese`方法：可利用正则表达式 '[\u4e00-\u9fa5，。、？！《》‘’“”：；]+' 匹配，然后对中文的meaningful segment进行切分，为作扩展，`TokenizeChinese` 方法提供了是否使用jieba分词及是否利用 *jieba* 的搜索引擎匹配模式的接口
   * *TextProcessor* 中的 `TokenizeMixedLan` 方法目的在于保留除中文字符与标点符号外的内容，但将中文替换为分词后的结果，为了节省空间，`TokenizeMixedLan` 方法比 `TokenizeChinese` 方法 多做的工作在循环中记录了上一次的分词终止的位置，然后将 last_tokenized 到当前分词开始处的内容保留，重复更新结果变量。
4.  *TextProcessing* 中的 `parseContents` 方法
    为加强文本与网页的相关性，这里使用了 *lxml.html.clean.Cleaner* 类，利用 `Cleaner(style=True, scripts=True, links = False, remove_tags = remove_tags)` 方法来定位到我们想要的内容，我清除掉了 footer, css样式，脚本文件（虽然无法排除有少数网页的HTML内容就放在script中进行动态渲染，或者以 Ajax 请求交换 JSON 数据来得到脚本渲染网页时的原材料）,同时我也只保留了body部分的网页内容。
5. parse 网页中的外链
   xpath中的写法为 `'//a[@href and @rel != "nofollow"] | //a[@href] | //iframe[@src]'` 这里不处理网页的 nofollow 标签，因为做的假设是这些 nofollow links对网页的贡献度不大，这里也就爬取时滤过，网页中的链接分好多种，这里只针对转向其他有效网页的外链，所以首先去除 `href:"javascript:void(...)`形式，其次判断是否是锚点，即是否startswith('#'),最后判断该链接是否是资源下载链接，作为另一种连接形式保存，这里使用正则表达式匹配后缀名实现。
6. 如果网页中有 `<base>` 标签，那么可以将其作为基准网页定位相对地址。
7. 编码问题：有些网站的 charset 与实际网页内容使用的编码方式不符，常见在某些 gbk 与 gb2312 为主的网站,所以要对 *UnicodeError* 进行异常捕获。


### 3.3.2 问题分析 ### 


&emsp;


### 3.3.3 代码与运行结果 ### 


 代码展示如下: 

```

```

运行结果如下：



#### 分析与反思： ####

&emsp;

## 4.拓展思考 ###

&emsp;

